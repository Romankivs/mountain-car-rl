# Mountain Car - Policy Iteration та Value Iteration

Реалізація алгоритмів **Policy Iteration** та **Value Iteration** для середовища Mountain Car з Gymnasium.

## Опис середовища

Mountain Car - це класична задача reinforcement learning, де машина намагається вибратися з долини на пагорб. Машина має недостатньо потужності, щоб просто поїхати вгору, тому вона повинна використовувати імпульс, розгойдуючись назад і вперед.

**Простір станів (неперервний):**
- Position: від -1.2 до 0.6
- Velocity: від -0.07 до 0.07

**Простір дій (дискретний):**
- 0: Прискорення вліво
- 1: Нічого не робити
- 2: Прискорення вправо

**Винагорода:**
- -1 за кожен крок
- Епізод закінчується, коли машина досягає позиції ≥ 0.5

## Особливості реалізації

### Дискретизація простору станів

Оскільки Policy Iteration та Value Iteration працюють з дискретними станами, ми дискретизуємо неперервний простір станів Mountain Car:

- Position розбивається на `n_position_bins` bins (за замовчуванням 20)
- Velocity розбивається на `n_velocity_bins` bins (за замовчуванням 20)
- Загальна кількість станів: 20 × 20 = 400

### Збір семплів переходів

Оскільки ми не маємо точної моделі динаміки середовища, ми збираємо семпли переходів:

1. Виконуємо епізоди з випадковими діями
2. Для кожного стану пробуємо всі можливі дії
3. Зберігаємо переходи (s, a, s', r)
4. Оцінюємо ймовірності переходів P(s'|s,a) та винагороди R(s,a,s')

## Установка

```bash
pip install gymnasium numpy matplotlib tqdm
```

## Використання

### Policy Iteration

```bash
python mountain_car_policy_iteration.py
```

**Основні параметри:**
```python
agent = MountainCarPolicyIteration(
    n_position_bins=20,    # Кількість bins для позиції
    n_velocity_bins=20,    # Кількість bins для швидкості
    gamma=0.99,            # Discount factor
    theta=1e-6             # Поріг збіжності
)

# Навчання
policy, V = agent.train(
    max_iterations=50,     # Максимум ітерацій
    n_samples=1000         # Кількість семплів для збору переходів
)

# Тестування
rewards, lengths = agent.test_policy(n_episodes=10, render=False)

# Візуалізація
agent.plot_results()
```

### Value Iteration

```bash
python mountain_car_value_iteration.py
```

**Основні параметри:**
```python
agent = MountainCarValueIteration(
    n_position_bins=20,    # Кількість bins для позиції
    n_velocity_bins=20,    # Кількість bins для швидкості
    gamma=0.99,            # Discount factor
    theta=1e-6             # Поріг збіжності
)

# Навчання
policy, V = agent.train(
    max_iterations=1000,   # Максимум ітерацій
    n_samples=1000         # Кількість семплів для збору переходів
)

# Тестування
rewards, lengths = agent.test_policy(n_episodes=10, render=False)

# Візуалізація
agent.plot_results()
```

## Алгоритми

### Policy Iteration

Policy Iteration працює в два кроки:

1. **Policy Evaluation**: Обчислення V(s) для поточної політики π
   ```
   V(s) = Σ P(s'|s,π(s)) * [R(s,π(s),s') + γ * V(s')]
   ```

2. **Policy Improvement**: Покращення політики на основі V(s)
   ```
   π(s) = argmax_a Σ P(s'|s,a) * [R(s,a,s') + γ * V(s')]
   ```

Алгоритм повторює ці кроки до тих пір, поки політика не стабілізується.

### Value Iteration

Value Iteration комбінує обидва кроки в один:

```
V(s) = max_a Σ P(s'|s,a) * [R(s,a,s') + γ * V(s')]
```

Після збіжності V(s), оптимальна політика витягується:
```
π*(s) = argmax_a Σ P(s'|s,a) * [R(s,a,s') + γ * V(s')]
```

## Візуалізація результатів

Обидва скрипти генерують графіки з результатами:

### Policy Iteration
- Збіжність Value Function
- Heatmap Value Function V(s)
- Heatmap політики
- Розподіл дій в політиці

### Value Iteration
- Збіжність Value Function
- Збіжність Delta (на лог-шкалі)
- Heatmap Value Function V(s)
- Heatmap політики
- Розподіл дій в політиці
- Розподіл значень V

## Порівняння алгоритмів

| Характеристика | Policy Iteration | Value Iteration |
|---------------|------------------|-----------------|
| Швидкість збіжності | Зазвичай менше ітерацій | Більше ітерацій, але простіші |
| Обчислювальна складність | Вища на ітерацію (повна policy evaluation) | Нижча на ітерацію |
| Збіжність | Гарантована до оптимальної політики | Гарантована до оптимальної V(s) |
| Проміжні політики | Можуть бути використані | Тільки фінальна |

## Очікувані результати

Через складність задачі Mountain Car для методів динамічного програмування:

- Навчання може зайняти кілька хвилин
- Середня винагорода: близько -200 (залежить від дискретизації)
- Епізод зазвичай триває 100-200 кроків
- Політика навчається використовувати імпульс для досягнення мети

## Покращення

Для кращих результатів можна:

1. **Збільшити дискретизацію**: більше bins = точніша апроксимація
2. **Зібрати більше семплів**: краща оцінка моделі переходів
3. **Налаштувати gamma**: вищі значення = більше врахування майбутніх винагород
4. **Використати priority sweep**: оновлювати тільки важливі стани

## Обмеження

- Дискретизація призводить до втрати точності
- Потрібно багато семплів для хорошої оцінки моделі
- Обчислювальна складність росте з кількістю станів
- Не масштабується на високорозмірні задачі

## Типові проблеми та рішення

### Проблема 1: Недостатньо унікальних переходів
**Симптом**: "Зібрано 9 унікальних пар (стан, дія)" замість очікуваних ~1200

**Причина**: Стандартний reset() завжди починає з однієї й тієї ж області, тому досліджується лише мала частина простору станів.

**Рішення**: Код тепер використовує випадкові початкові стани для кращого покриття простору станів.

### Проблема 2: Політика не вчиться досягати мети
**Симптом**: Всі епізоди закінчуються на 200 кроках з винагородою -200

**Причина**: 
1. Недостатньо семплів переходів
2. Занадто груба дискретизація
3. Mountain Car - складна задача для табличних методів

**Рішення**:
```python
agent = MountainCarValueIteration(
    n_position_bins=40,    # Збільшена дискретизація
    n_velocity_bins=40,
    gamma=0.95,            # Менший gamma
    theta=1e-4
)
policy, V = agent.train(
    max_iterations=2000,
    n_samples=5000         # Більше семплів!
)
```

### Проблема 3: FileNotFoundError при збереженні графіків
**Симптом**: `/home/claude/...png` не знайдено

**Рішення**: Код оновлено для збереження у поточну директорію.

## Альтернативи

Для кращої продуктивності на Mountain Car рекомендується використовувати:
- Q-Learning
- SARSA
- Deep Q-Networks (DQN)
- Policy Gradient методи

Ці методи не потребують дискретизації та працюють безпосередньо з неперервним простором станів.

## Автор

Реалізація для навчальних цілей.

## Ліцензія

MIT
# mountain-car-rl
